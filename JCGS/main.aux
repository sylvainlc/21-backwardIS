\relax 
\citation{michelot2016movehmm}
\citation{candanedo2017methodology}
\citation{yau2011bayesian,gassiat2016inference,wang2017variational}
\citation{sarkka2007rao}
\citation{rabiner1989tutorial}
\citation{sarkka2013bayesian,douc2014nonlinear,zucchini2017hidden}
\citation{dempster1977maximum}
\citation{cappe2005inference}
\citation{douc2011sequential}
\citation{briers2010smoothing,fearnhead2010sequential,nguyen2017two}
\citation{delmoral2010backward,douc2011sequential,dubarry2013nonasymptotic,gerber2017convergence}
\citation{olsson2017efficient}
\citation{gordon1993novel}
\citation{olsson2020particle}
\citation{andersson2017unbiased,fearnhead2017continuous}
\citation{martin:jasra:singh:whiteley:delmoral:maccoy:2014}
\citation{fearnhead2010random}
\citation{yonekura:beskos:2020}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\newlabel{sec:intro}{{1}{2}}
\citation{olsson2017efficient}
\citation{gloaguen2018online}
\citation{gloaguen2021pseudo}
\citation{gloaguen2021pseudo}
\citation{beskos2006retrospective}
\citation{fearnhead2008particle}
\citation{andersson2017unbiased}
\citation{fearnhead2017continuous}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model and objectives}{5}}
\newlabel{sec:model}{{2}{5}}
\newlabel{eq:def:elln}{{1}{5}}
\newlabel{eq:additive:functional}{{2}{5}}
\newlabel{ex:state:tracking}{{1}{5}}
\citation{dempster1977maximum}
\citation{cappe2005inference}
\citation{cappe2005inference}
\citation{olsson2020particle}
\newlabel{ex:em:algorithm}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Online sequential Monte Carlo smoothing}{6}}
\newlabel{sec:method}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Backward statistic for online smoothing}{6}}
\citation{fearnhead2008particle,olsson2011particle,gloaguen2018online,gloaguen2021pseudo}
\citation{andrieu2009pseudo}
\citation{gloaguen2021pseudo}
\citation{legland1997recursive}
\newlabel{eq:T:stat}{{3}{7}}
\newlabel{eq:property:filt:smooth}{{4}{7}}
\newlabel{eq:property:backward}{{5}{7}}
\newlabel{assum:unbiased}{{1}{7}}
\citation{cappe2005inference}
\citation{delmoral2015uniform}
\citation{olsson2020particle}
\newlabel{eq:online:gradient}{{6}{8}}
\newlabel{eq:complete:gradient:log}{{7}{8}}
\newlabel{eq:tangent:identity}{{8}{8}}
\citation{legland1997recursive}
\citation{tadic2010analyticity}
\newlabel{eq:par:update}{{9}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Approximation of the filtering distribution}{9}}
\newlabel{sec:filtering}{{3.2}{9}}
\citation{gloaguen2021pseudo}
\citation{gloaguen2018online}
\newlabel{eq:weight-update-filtering}{{10}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Approximation of the backward statistics}{10}}
\newlabel{sec:smoothing}{{3.3}{10}}
\newlabel{eq:update:paris:marginal}{{11}{10}}
\newlabel{eq:AR:bound}{{12}{11}}
\citation{fearnhead2010random}
\newlabel{eq:tangent:identity:part:linear}{{13}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Pseudo-marginal backward importance sampling}{12}}
\newlabel{sec:backwardis}{{4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Positive estimates}{12}}
\newlabel{sec:wald:trick}{{4.1}{12}}
\@writefile{toc}{\contentsline {paragraph}{Particle filtering weights.}{12}}
\citation{Mozer1989AFB}
\citation{Hochreiter1997LongSM,Cho2014LearningPR}
\citation{mikolov2010recurrent,sutskever2011generating,sutskever2014sequence}
\@writefile{toc}{\contentsline {paragraph}{Backward simulation weights.}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}AR-free online smoothing}{13}}
\newlabel{eq:update:paris:marginal:is}{{14}{13}}
\citation{Hochreiter1997LongSM}
\citation{Cho2014LearningPR}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application to smoothing expectations and score estimation}{14}}
\newlabel{sec:application}{{5}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Recurrent neural networks}{14}}
\newlabel{sec:simu:RNN}{{5.1}{14}}
\citation{douc2014nonlinear}
\citation{beskos2006exact}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Empirical estimation of $\mathbb  {E}[\delimiter "026B30D X_0 - \mathbb  {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle E$}\mathaccent "0362{E}}[X_0|Y_{0:n}]\delimiter "026B30D ^2]$ and $\mathbb  {E}[ \DOTSB \sum@ \slimits@ _{k=0}^{n}\delimiter "026B30D X_k - \mathbb  {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle E$}\mathaccent "0362{E}}[X_k|Y_{0:n}]\delimiter "026B30D ^2/(n+1)]$ for the Poor man's smoother (PMS) and the Backward IS smoother, for states $X_{0:n}$ and observations $Y_{0:n}$ generated with stochastic RNNs of dimension 32 and 64. We consider an online estimation of a sequence of 200 observations, truncated at timestep $n=49$ (50 timesteps), $n=99$ (100 timesteps) and the full sequence ($n=199$).}}{16}}
\newlabel{table:RNN_exp}{{1}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}One dimensional diffusion processe: the Sine model}{16}}
\newlabel{sec:simu:SINE}{{5.2}{16}}
\newlabel{eq:obs:model:SINE}{{15}{16}}
\citation{olsson2017efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of the empirical estimate of $\mathbb  {E}[\delimiter "026B30D X_k - \mathbb  {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle E$}\mathaccent "0362{E}}[X_k|Y_{0:199}]\delimiter "026B30D ^2]$ for $k \in \{0,...,199\}$ for 100 runs of the Backward IS and Poor Man smoothers.}}{17}}
\newlabel{fig:RNN:mseperXk}{{1}{17}}
\newlabel{eq:optimal:filter}{{16}{17}}
\citation{beskos2006retrospective}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Computational complexity and estimation of a posterior mean as a function of the number of backward samples. Results are shown for the state of the art acceptance-rejection (AR) algorithm and the proposed backward importance sampling (IS) technique.}}{18}}
\newlabel{fig:sine:timeandbias}{{2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Recursive maximum likelihood estimation in the Sine model}{18}}
\newlabel{sec:simu:tangent:filter}{{5.3}{18}}
\citation{polyak1992acceleration,kushner1997stochastic}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Computational complexity and estimation of a posterior mean as a function of the number of particles. Results are shown for the state of the art acceptance-rejection (AR) algorithm and the proposed backward importance sampling (IS) technique. The number of backward samples is set to 2 for the AR, and to $N^{0.5}, N^{0.6}$ and $N/10$ for the IS.}}{19}}
\newlabel{fig:sine:timeandbias:N:vary}{{3}{19}}
\citation{gloaguen2021pseudo}
\citation{hening2018persistence}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (\textit  {Left}) Data set simulated according to the SINE process, observed with noise at discrete time steps. (\textit  {Middle}) Gradient step sizes (defined in equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 9\hbox {}\unskip \@@italiccorr )}}) for online estimation. (\textit  {Right}) Online estimation of $\theta $. Colors differentiate the 50 different starting points. The red horizontal line shows the true value.}}{20}}
\newlabel{fig:1obs:50start}{{4}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Multidimensional diffusion processes: Stochastic Lotka-Volterra model}{20}}
\newlabel{sec:simu:LV}{{5.4}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (\textit  {Left}) online estimation of $\theta $ for 50 different simulated data sets. The algorithm is performed from 1 starting point with the gradient step size shown in Figure 4\hbox {}. (\textit  {Middle}) Averaged estimator, where $\mathaccentV {hat}05E{\theta }$ is averaged after a burning phase of 300 time steps. (\textit  {Right}) Empirical distribution of $\mathaccentV {hat}05E{\theta }$. The red line is the value of $\theta ^*$.}}{21}}
\newlabel{fig:50obs:1start}{{5}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (\textit  {Left}) online estimation of $\theta $ for the data set presented in Figure 4\hbox {}, with different decreasing rates values $\kappa $. (\textit  {Right}) Averaged estimator, where $\mathaccentV {hat}05E{\theta }$ is averaged after a burning phase of 300 time steps.}}{21}}
\newlabel{fig:1obs:1start:6Grads}{{6}{21}}
\citation{fearnhead2017continuous}
\citation{odum1971fundamentals}
\citation{dempster1977maximum}
\citation{hansen2006cma}
\newlabel{eq:LV:SDE}{{17}{22}}
\newlabel{eq:LV:obs:model}{{18}{22}}
\bibstyle{Chicago}
\bibdata{references}
\bibcite{ait-sahalia2008closed}{{1}{2008}{{A\"{i}t-Shalia}}{{A\"{i}t-Shalia}}}
\bibcite{andersson2017unbiased}{{2}{2017}{{Andersson and Kohatsu-Higa}}{{Andersson and Kohatsu-Higa}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Estimated Hares-Lynx abundances using the Hudson bay company data set. Both our IS-PaRIS smoother and the poor man smoother are performed to approximate the MLE and solve the tracking problem. Blue crosses show the observations.}}{23}}
\newlabel{fig:LV:hares:lynx}{{7}{23}}
\bibcite{andrieu2009pseudo}{{3}{2009}{{Andrieu and Roberts}}{{Andrieu and Roberts}}}
\bibcite{beskos2006retrospective}{{4}{2006}{{Beskos et~al.}}{{Beskos, Papaspiliopoulos, and Roberts}}}
\bibcite{beskos2006exact}{{5}{2006}{{Beskos et~al.}}{{Beskos, Papaspiliopoulos, Roberts, and Fearnhead}}}
\bibcite{briers2010smoothing}{{6}{2010}{{Briers et~al.}}{{Briers, Doucet, and Maskell}}}
\bibcite{candanedo2017methodology}{{7}{2017}{{Candanedo et~al.}}{{Candanedo, Feldheim, and Deramaix}}}
\bibcite{cappe2005inference}{{8}{2005}{{Capp\'e et~al.}}{{Capp\'e, Moulines, and Ryd\'en}}}
\bibcite{Cho2014LearningPR}{{9}{2014}{{Cho et~al.}}{{Cho, van MerriÃ«nboer, Gulcehre, Bougares, Schwenk, and Bengio}}}
\bibcite{delmoral2010backward}{{10}{2010}{{Del~Moral et~al.}}{{Del~Moral, Doucet, and Singh}}}
\bibcite{delmoral2015uniform}{{11}{2015}{{Del~Moral et~al.}}{{Del~Moral, Doucet, and Singh}}}
\bibcite{dempster1977maximum}{{12}{1977}{{Dempster et~al.}}{{Dempster, Laird, and Rubin}}}
\bibcite{douc2011sequential}{{13}{2011}{{Douc et~al.}}{{Douc, Garivier, Moulines, and Olsson}}}
\bibcite{douc2014nonlinear}{{14}{2014}{{Douc et~al.}}{{Douc, Moulines, and Stoffer}}}
\bibcite{dubarry2013nonasymptotic}{{15}{2013}{{Dubarry and Le~Corff}}{{Dubarry and Le~Corff}}}
\bibcite{fearnhead2017continuous}{{16}{2017}{{Fearnhead et~al.}}{{Fearnhead, Latuszynski, Roberts, and Sermaidis}}}
\bibcite{fearnhead2010random}{{17}{2010}{{Fearnhead et~al.}}{{Fearnhead, Papaspiliopoulos, Roberts, and Stuart}}}
\bibcite{fearnhead2008particle}{{18}{2008}{{Fearnhead et~al.}}{{Fearnhead, Papaspiliopoulos, and Roberts}}}
\bibcite{fearnhead2010sequential}{{19}{2010}{{Fearnhead et~al.}}{{Fearnhead, Wyncoll, and Tawn}}}
\bibcite{gassiat2016inference}{{20}{2016}{{Gassiat et~al.}}{{Gassiat, Cleynen, and Robin}}}
\bibcite{gerber2017convergence}{{21}{2017}{{Gerber and Chopin}}{{Gerber and Chopin}}}
\bibcite{gloaguen2018online}{{22}{2018}{{Gloaguen et~al.}}{{Gloaguen, Etienne, and Le~Corff}}}
\bibcite{gloaguen2021pseudo}{{23}{2021}{{Gloaguen et~al.}}{{Gloaguen, Le~Corff, and Olsson}}}
\bibcite{gordon1993novel}{{24}{1993}{{Gordon et~al.}}{{Gordon, Salmond, and Smith}}}
\bibcite{hansen2006cma}{{25}{2006}{{Hansen}}{{Hansen}}}
\bibcite{hening2018persistence}{{26}{2018}{{Hening and Nguyen}}{{Hening and Nguyen}}}
\bibcite{Hochreiter1997LongSM}{{27}{1997}{{Hochreiter and Schmidhuber}}{{Hochreiter and Schmidhuber}}}
\bibcite{kushner1997stochastic}{{28}{1997}{{Kushner and Yin}}{{Kushner and Yin}}}
\bibcite{legland1997recursive}{{29}{1997}{{{Le~Gland} and Mevel}}{{{Le~Gland} and Mevel}}}
\bibcite{martin:jasra:singh:whiteley:delmoral:maccoy:2014}{{30}{2014}{{Martin et~al.}}{{Martin, Jasra, Singh, Whiteley, {D}el {M}oral, and Mc{C}oy}}}
\bibcite{michelot2016movehmm}{{31}{2016}{{Michelot et~al.}}{{Michelot, Langrock, and Patterson}}}
\bibcite{mikolov2010recurrent}{{32}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, {\v {C}}ernock{\`y}, and Khudanpur}}}
\bibcite{Mozer1989AFB}{{33}{1989}{{Mozer}}{{Mozer}}}
\bibcite{nguyen2017two}{{34}{2017}{{Nguyen et~al.}}{{Nguyen, Le~Corff, and Moulines}}}
\bibcite{odum1971fundamentals}{{35}{1971}{{Odum and Barrett}}{{Odum and Barrett}}}
\bibcite{olsson2020particle}{{36}{2020}{{Olsson and Alenl{\"o}v}}{{Olsson and Alenl{\"o}v}}}
\bibcite{olsson2011particle}{{37}{2011}{{Olsson et~al.}}{{Olsson, Str{\"o}jby, et~al.}}}
\bibcite{olsson2017efficient}{{38}{2017}{{Olsson et~al.}}{{Olsson, Westerborn, et~al.}}}
\bibcite{polyak1992acceleration}{{39}{1992}{{Polyak and Juditsky}}{{Polyak and Juditsky}}}
\bibcite{rabiner1989tutorial}{{40}{1989}{{Rabiner}}{{Rabiner}}}
\bibcite{sarkka2013bayesian}{{41}{2013}{{S\"arkk\"a}}{{S\"arkk\"a}}}
\bibcite{sarkka2007rao}{{42}{2007}{{S\"arkk\"a et~al.}}{{S\"arkk\"a, Vehtari, and Lampinen}}}
\bibcite{sutskever2011generating}{{43}{2011}{{Sutskever et~al.}}{{Sutskever, Martens, and Hinton}}}
\bibcite{sutskever2014sequence}{{44}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{tadic2010analyticity}{{45}{2010}{{Tadi\'c}}{{Tadi\'c}}}
\bibcite{wang2017variational}{{46}{2017}{{Wang et~al.}}{{Wang, Lebarbier, Aubert, and Robin}}}
\bibcite{yau2011bayesian}{{47}{2011}{{Yau et~al.}}{{Yau, Papaspiliopoulos, Roberts, and Holmes}}}
\bibcite{yonekura:beskos:2020}{{48}{2020}{{Yonekura and Beskos}}{{Yonekura and Beskos}}}
\bibcite{zucchini2017hidden}{{49}{2017}{{Zucchini et~al.}}{{Zucchini, Mac~Donald, and Langrock}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Application to partially observed SDE}{28}}
\newlabel{sec:filter:SDE}{{A}{28}}
\newlabel{eq:sde}{{19}{28}}
\citation{olsson2011particle}
\citation{gloaguen2018online}
\citation{beskos2006retrospective}
\citation{ait-sahalia2008closed}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Unbiased estimators of the transition densities}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}General Poisson Estimators}{29}}
\newlabel{eq:q:girsanov}{{20}{29}}
\citation{fearnhead2008particle}
\citation{beskos2006retrospective}
\@writefile{toc}{\contentsline {paragraph}{Unbiased GPE estimator for $q_{k+1;\theta }(x,y;\zeta )$.}{30}}
\@writefile{toc}{\contentsline {paragraph}{Unbiased GPE estimator of $\nabla _{\theta }\qopname  \relax o{log}q_{k+1;\theta }(x,y)$.}{30}}
\citation{andersson2017unbiased}
\citation{fearnhead2017continuous}
\citation{fearnhead2017continuous}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Parametrix estimators}{31}}
\newlabel{CIS:eq:rho}{{21}{31}}
\citation{fearnhead2017continuous}
\@writefile{toc}{\contentsline {section}{\numberline {B}Supplementary material for Section 5.4\hbox {}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Estimated predator-prey abundances (middle) in a stochastic Lotka Volterra model using our backward sampling estimate on simulated abundance indexes (left). Right panel shows the ground truth.}}{34}}
\newlabel{fig:LV:tracking}{{8}{34}}
