\relax 
\citation{michelot2016movehmm}
\citation{candanedo2017methodology}
\citation{yau2011bayesian}
\citation{gassiat2016inference}
\citation{wang2017variational}
\citation{sarkka2007rao}
\citation{rabiner1989tutorial}
\citation{sarkka2013bayesian}
\citation{douc2014nonlinear}
\citation{zucchini2017hidden}
\citation{dempster1977maximum}
\citation{cappe2005inference}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{gloaguen2019pseudo}
\citation{gloaguen2019pseudo}
\citation{olsson2017efficient}
\citation{douc2011sequential}
\citation{briers2010smoothing}
\citation{fearnhead2010sequential}
\citation{nguyen2017two}
\citation{gloaguen2019pseudo}
\citation{olsson2017efficient}
\citation{gordon1993novel}
\citation{delmoral2010backward}
\citation{douc2011sequential}
\citation{dubarry2013nonasymptotic}
\citation{gerber2017convergence}
\citation{beskos2006retrospective}
\citation{fearnhead2008particle}
\citation{andersson2017unbiased}
\citation{fearnhead2017continuous}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model and objectives}{2}}
\newlabel{sec:model}{{2}{2}}
\citation{cappe2005inference}
\citation{gloaguen2019pseudo}
\citation{cappe2005inference}
\newlabel{eq:EM:E:step}{{1}{3}}
\newlabel{eq:smooth}{{2}{3}}
\citation{doucet2013sequential}
\citation{gloaguen2018online}
\citation{beaumont:2003}
\citation{andrieu:robert:2009}
\citation{fearnhead2008particle}
\citation{olsson2011particle}
\citation{gloaguen2018online}
\citation{gloaguen2019pseudo}
\newlabel{eq:T:stat}{{3}{4}}
\newlabel{eq:T:add:recursion}{{4}{4}}
\newlabel{eq:paris:estimator}{{5}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Online sequential Monte Carlo smoothing}{4}}
\newlabel{sec:bayesian:smoothing}{{3}{4}}
\newlabel{assum:unbiased}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Filtering}{4}}
\newlabel{sec:filtering}{{3.1}{4}}
\citation{delmoral2010backward}
\citation{olsson2017efficient}
\citation{douc2011sequential}
\citation{dubarry2011fast}
\citation{gloaguen2019pseudo}
\citation{gloaguen2018online}
\newlabel{eq:weight-update-filtering}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Smoothing}{5}}
\newlabel{sec:AR}{{3.2}{5}}
\citation{fearnhead2010random}
\newlabel{eq:AR:bound}{{7}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Pseudo-marginal backward importance sampling}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Positive transition densities estimates}{6}}
\@writefile{toc}{\contentsline {paragraph}{Particle filtering.}{6}}
\citation{beskos2006exact}
\@writefile{toc}{\contentsline {paragraph}{Backward simulation.}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}AR-free online smoothing}{7}}
\newlabel{eq:update:paris:marginal:is}{{8}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application to smoothing expectations and score estimation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sine model}{7}}
\citation{olsson2017efficient}
\citation{hening2018persistence}
\newlabel{eq:obs:model:SINE}{{9}{8}}
\newlabel{eq:optimal:filter}{{10}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stochastic Lotka-Volterra model}{8}}
\newlabel{eq:LV:SDE}{{11}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Computational complexity and estimation of a posterior mean as a function of the number of backward samples. Results are shown for the state of the art acceptance-rejection algorithm and the proposed backward importance sampling technique.}}{9}}
\newlabel{fig:sine:timeandbias}{{1}{9}}
\newlabel{eq:LV:obs:model}{{12}{9}}
\citation{odum1971fundamentals}
\citation{dempster1977maximum}
\citation{hansen2006cma}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Computational complexity and estimation of a posterior mean as a function of the number of particles. Results are shown for the state of the art acceptance-rejection algorithm and the proposed backward importance sampling technique. The number of backward samples is set to 2 for the AR, and $N/10$ for the IS.}}{10}}
\newlabel{fig:sine:timeandbias:N:vary}{{2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Estimated predator-prey abundances (center) in a stochastic Lotka Volterra model using our backward sampling estimate on simulated abundance indexes (left). Right panel shows the ground truth.}}{11}}
\newlabel{fig:LV:tracking}{{3}{11}}
\citation{delmoral:doucet:singh:2015}
\citation{olsson:westerborn:2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Recurrent neural networks}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Tangent filters and online recursive maximum likelihood}{12}}
\newlabel{sec:tangent:filter}{{6}{12}}
\citation{cappe:moulines:ryden:2005}
\citation{douc:matias:2001}
\citation{olsson:westerborn:2017}
\newlabel{eq:tangent:identity}{{13}{13}}
\newlabel{eq:tangent:identity:part}{{14}{13}}
\newlabel{eq:tangent:identity:part:linear}{{15}{13}}
\newlabel{eq:par:update}{{16}{13}}
\newlabel{eq:def:zeta}{{17}{13}}
\citation{legland:mevel:1997}
\citation{tadic:2010}
\citation{olsson2011particle}
\citation{gloaguen2018online}
\citation{gloaguen2019pseudo}
\citation{beskos2006retrospective}
\citation{ait-sahalia2008closed}
\citation{andersson2017unbiased}
\citation{fearnhead2017continuous}
\newlabel{eq:zeta:terms}{{18}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Application to partially observed SDE}{14}}
\newlabel{sec:filter:SDE}{{6.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Unbiased estimators of the transition densities}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}General Poisson Estimators}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Parametrix estimators}{14}}
\citation{fearnhead2017continuous}
\citation{fearnhead2017continuous}
\citation{dacunha1986estimation}
\newlabel{CIS:eq:rho}{{19}{15}}
\newlabel{eq:sde}{{20}{15}}
\citation{fearnhead:papaspiliopoulos:roberts:2008}
\citation{fearnhead:papaspiliopoulos:roberts:2008}
\citation{beskos2006retrospective}
\newlabel{eq:q:girsanov}{{21}{16}}
\@writefile{toc}{\contentsline {paragraph}{Unbiased GPE estimator for $q_{k+1;\theta }(x,y;\zeta )$.}{16}}
\@writefile{toc}{\contentsline {paragraph}{Unbiased GPE estimator of $\nabla _{\theta }\qopname  \relax o{log}q_{k+1;\theta }(x,y)$.}{16}}
\citation{beskos2006retrospective}
\citation{gloaguen:etienne:lecorff:2018}
\citation{polyak:juditsky:1992}
\citation{kushner:yin:1997}
\@writefile{toc}{\contentsline {paragraph}{Experiments.}{17}}
\newlabel{eq:SINE}{{22}{17}}
\citation{andersson2017unbiased}
\citation{fearnhead2017continuous}
\citation{gloaguen2019pseudo}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Data set simulated according to the SINE process, observed with noise at discrete time steps.}}{18}}
\newlabel{fig:data}{{4}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{18}}
\newlabel{sec:discussion}{{7}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (\textit  {Left}) online estimation of $\theta $ for the data set presented in Figure~\ref  {fig:data}. The algorithm is performed from 50 starting points. (\textit  {Right}) The gradient step sizes (defined in equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:par:update}\unskip \@@italiccorr )}}).}}{19}}
\newlabel{fig:1obs:50start}{{5}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (\textit  {Left}) online estimation of $\theta $ for 50 different simulated data sets as presented in Figure \ref  {fig:data}. The algorithm is performed from 1 starting point with the gradient step size shown in Figure \ref  {fig:1obs:50start}. (\textit  {Center}) Averaged estimator, where $\mathaccentV {hat}05E{\theta }$ is averaged after a burning phase of 300 time steps. (\textit  {Right}) Empirical distribution of $\mathaccentV {hat}05E{\theta }$. The red line is the value of $\theta ^*$.}}{19}}
\newlabel{fig:50obs:1start}{{6}{19}}
\bibstyle{apalike}
\bibdata{backwardIS}
\bibcite{ait-sahalia2008closed}{A\"{i}t-Shalia, 2008}
\bibcite{andersson2017unbiased}{Andersson and Kohatsu-Higa, 2017}
\bibcite{beskos2006retrospective}{Beskos et~al., 2006a}
\bibcite{beskos2006exact}{Beskos et~al., 2006b}
\bibcite{briers2010smoothing}{Briers et~al., 2010}
\bibcite{candanedo2017methodology}{Candanedo et~al., 2017}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (\textit  {Left}) online estimation of $\theta $ for the data set presented in Figure \ref  {fig:data}, with different decreasing rates values $\kappa $. (\textit  {Right}) Averaged estimator, where $\mathaccentV {hat}05E{\theta }$ is averaged after a burning phase of 300 time steps.}}{20}}
\newlabel{fig:1obs:1start:6Grads}{{7}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Estimated Hares-Lynx abundances using the Hudson bay company data set. Both our IS-PaRIS smoother and the poor man smoother are performed to approximate the MLE and solve the tracking problem. Blue crosses show the observations.}}{21}}
\newlabel{fig:LV:hares:lynx}{{8}{21}}
\bibcite{cappe2005inference}{Capp\'e et~al., 2005}
\bibcite{delmoral2010backward}{Del~Moral et~al., 2010}
\bibcite{dempster1977maximum}{Dempster et~al., 1977}
\bibcite{douc2011sequential}{Douc et~al., 2011}
\bibcite{douc2014nonlinear}{Douc et~al., 2014}
\bibcite{doucet2013sequential}{Doucet et~al., 2013}
\bibcite{dubarry2011fast}{Dubarry and Le~Corff, 2011}
\bibcite{dubarry2013nonasymptotic}{Dubarry and Le~Corff, 2013}
\bibcite{fearnhead2017continuous}{Fearnhead et~al., 2017}
\bibcite{fearnhead2010random}{Fearnhead et~al., 2010a}
\bibcite{fearnhead2008particle}{Fearnhead et~al., 2008}
\bibcite{fearnhead2010sequential}{Fearnhead et~al., 2010b}
\bibcite{gassiat2016inference}{Gassiat et~al., 2016}
\bibcite{gerber2017convergence}{Gerber and Chopin, 2017}
\bibcite{gloaguen2018online}{Gloaguen et~al., 2018}
\bibcite{gloaguen2019pseudo}{Gloaguen et~al., 2019}
\bibcite{gordon1993novel}{Gordon et~al., 1993}
\bibcite{hansen2006cma}{Hansen, 2006}
\bibcite{hening2018persistence}{Hening and Nguyen, 2018}
\bibcite{michelot2016movehmm}{Michelot et~al., 2016}
\bibcite{nguyen2017two}{Nguyen et~al., 2017}
\bibcite{odum1971fundamentals}{Odum and Barrett, 1971}
\bibcite{olsson2011particle}{Olsson et~al., 2011}
\bibcite{olsson2017efficient}{Olsson et~al., 2017}
\bibcite{rabiner1989tutorial}{Rabiner, 1989}
\bibcite{sarkka2013bayesian}{S\"arkk\"a, 2013}
\bibcite{sarkka2007rao}{S\"arkk\"a et~al., 2007}
\bibcite{wang2017variational}{Wang et~al., 2017}
\bibcite{yau2011bayesian}{Yau et~al., 2011}
\bibcite{zucchini2017hidden}{Zucchini et~al., 2017}
